{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## give the continuous columns:\n",
    "## Please note that this is a inbuilt dataset in tensorFlow model\n",
    "## credits: https://www.tensorflow.org/tutorials/wide_and_deep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_file.csv\")\n",
    "\n",
    "continous_features = [];\n",
    "for each_column in train_df.columns:\n",
    "    if train_df[each_column].dtype == 'float':\n",
    "        each_column = tf.feature_column.numeric_column(each_column)\n",
    "        continous_features.append(each_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give the categorical columns and the base columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_features = []; base_columns = []\n",
    "for each_column in train_df.columns:\n",
    "    if train_df[each_column].dtype == 'str' or train_df[each_column].dtype == 'category':\n",
    "        base_columns.append(each_column)\n",
    "        each_column = tf.feature_column.embedding_column(each_column, dimension=8) #dimension is tunable\n",
    "        cat_features.append(each_column)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossed Columns: On combining features into a single feature, the model will be able to earn seperate weights for the combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_column_creator(base_columns):\n",
    "    \n",
    "    cross_columns\n",
    "    for i in range (len(base_columns)):\n",
    "        cross_columns.append(tf.feature_column.crossed_column(\n",
    "        [base_columns[i], base_columns[i+1]], hash_bucket_size=1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all the columns to create the deep columns list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_column = []\n",
    "for each_column in cat_features:\n",
    "    deep_column.append(each_column)\n",
    "for each_column in continous_features:\n",
    "    deep_column.append(each_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tf.estimator.DNNLinearCombinedRegressor(\n",
    "    model_dir='path',\n",
    "    linear_feature_columns=base_columns + crossed_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[1, 2])#tunable units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(iterations):\n",
    "  model.train(dnn_optimizer=tf.train.ProximalAdagradOptimizer(...),tunable_params)\n",
    "results = model.evaluate(input_fn=lambda: input_fn(\n",
    "      FLAGS.test_data, 1, False, FLAGS.batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating place holders for the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32,name = 'X')\n",
    "    Y = tf.placeholder(tf.float32,name = 'Y')\n",
    "   \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialization of Parameters for every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. \n",
    "            \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                  \n",
    "        \n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(len(layer_dims)):\n",
    "        parameters[\"W\"+str(l+1)] = tf.get_variable(\"W\", [l,l+1], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        parameters[\"b\"+str(l+1)] = tf.get_variable(\"b\", [l,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the forward propogation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters,number_of_layers,Y):   \n",
    "    \n",
    "    temp = X; Z= None;\n",
    "    for l in range(number_of_layers):\n",
    "        Z = tf.add(tf.matmul(parameters['W'+str((l+1))],temp),'b'+str((l+1)))\n",
    "        A = tf.nn.relu('Z'+str(l+1))\n",
    "        temp = A\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.square(Z - Y))\n",
    "    \n",
    "    return Z, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,layer_dims,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \n",
    "    \n",
    "    ops.reset_default_graph()                         \n",
    "                                      \n",
    "    (n_x, m) = X_train.shape                          \n",
    "    n_y = Y_train.shape[0]                            \n",
    "    costs = []                                       \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    parameters = initialize_parameters (layer_dims)\n",
    "    Z = forward_propagation(X, parameters)[0]\n",
    "    cost = forward_propagation(X, parameters)[1]\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       \n",
    "            num_minibatches = int(m / minibatch_size) \n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
